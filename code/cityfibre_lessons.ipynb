{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CityFibre Data Analysis Challenge - python version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "Where the code contains **s around the text, that is where learners will need to change the text to put in their own values to make it work. eg \"\\*\\*inputdataset\\*\\*\" would need to be exchanged for a dataset name.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 1 - Environment setup\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 1a - Setting up the environment**\n",
    "\n",
    "This first lesson covers setting up then python environment correctly for the required analysis.\n",
    "    \n",
    "This involves:\n",
    "\n",
    "* Setting up the computer environment with the required packages\n",
    "* Creating links in to the folders containing the input and output datasets\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Set up the environment\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Python packages are a collection of definitions and functions developed by the open source community. They make it quicker to undertake analysis by providing additional functionality for common activities on top of base python's functionality.\n",
    "\n",
    "There are thousands of packages available to choose from on [PyPI](https://pypi.org) - The Python Package Index).\n",
    "\n",
    "One option to access python is to use the [Anaconda](https://www.anaconda.com/products/individual#Downloads) installation, which is a data science platform that integrates a lot of tools for data science work.\n",
    "    \n",
    "iPython notebooks can be used through either JupyterLab or Jupyter notebooks.\n",
    "    \n",
    "Another option is work entirely online utilising [Google Colaboratory (Colab)](https://colab.research.google.com/). This provides a consistent environment that integrates with a user's Google account.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install python packages\n",
    "\n",
    "The packages required for this analysis exercise are:\n",
    "\n",
    "* [pandas](https://pandas.pydata.org): A widely used python data analysis and data manipulation tool\n",
    "* [NumPy](https://numpy.org): Collects a lot of functions for scientific computing based on its superfast n-dimentional arrays\n",
    "* [Pyjanitor](https://pyjanitor.readthedocs.io): Extends pandas capabilities in cleaning data\n",
    "* [GeoPandas](https://geopandas.org): Extends pandas capabilities in handling geospatial data\n",
    "* [folium](https://python-visualization.github.io/folium/): Used to create interactive maps\n",
    "* [seaborn](https://seaborn.pydata.org/): A python plotting library \n",
    "* [matplotlib](https://matplotlib.org/): A python plotting library\n",
    "\n",
    "To use these packages they need to be specified in your environment first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the environment, different packages will be installed by default and different commands will be needed to install the required packages.\n",
    "\n",
    "If using the **Anaconda** environment, then packages are installed with the command: \n",
    "    `conda install *packagename*`\n",
    "    \n",
    "If using another python environment, then packages are normally installed with the command:\n",
    "    `pip install *packagename*`\n",
    "    \n",
    "If using Google Colab, then an additional `!` is required:\n",
    "    `!pip install *packagename*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules\n",
    "\n",
    "A package is a collection of modules, to actually use our packages, we then need to import modules from these packages using import statements.\n",
    "\n",
    "The import statement in python goes: <code>import _module\\_name_ as _module\\_alias_</code>.\n",
    "People usually use `pd`, `gpd`, and `np` to rename `pandas`, `geopandas` and `numpy` module respectively.\n",
    "\n",
    "We also need module `os.path`, which contains many operating system (including file system) related functions to help us manipulate file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import janitor\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Modules are often referred to by their aliases (i.e. `pd`, `np` and `path`).\n",
    "\n",
    "The dot syntax is used to refer a member under the name of a module / object type. i.e. <code>_module\\_name_._member\\_name_</code>, <code>_object\\_type\\_name_._member\\_name_</code>.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- `pd.read_csv()`: the `read_csv()` function under module name `pd`.\n",
    "- `DataFrame.shape`: the `shape` attribute of `DataFrame` objects\n",
    "- `DataFrame.head()`: the `head()` method of `DataFrame` objects\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the file path\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Some of the required input datasets are in a specific directory, called `data`.\n",
    "    \n",
    "All additional downloaded datasets should also be added to this folder.\n",
    "\n",
    "It is necessary to find this folder and set up a variable that points directly to this folder.\n",
    "All the input datasets can now be imported directly from this folder.\n",
    "\n",
    "</div>\n",
    "\n",
    "Assign the `input_file_path` to be the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " input_file_path = \"**~/myprojectpath/data/**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using **Google Colab** it is necessary to first mount the google drive before being able to reference its location.\n",
    "Uncomment the code snippet below to do this.\n",
    "It will require authorisation from a google account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also create an output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"**~/myprojectpath/data/**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 2 - Dataset understanding\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 2 - Data understanding**\n",
    "\n",
    "This second lesson covers the basic tasks to become familiar with the datasets being used.\n",
    "    \n",
    "This involves:\n",
    "    \n",
    "* Reading in the datasets.\n",
    "* Tidying up the variable names in each dataset so that they are easier to work with.\n",
    "* Acquire an initial understanding of each dataset size and shape and the types and format of the data that they contain.\n",
    "* Identify any obvious data quality problems that might need to be addressed.\n",
    "* Finally, gain an insight into how the columns are related to each other.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the CityFibre dataset\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The initial dataset provided by CityFibre is called `cityfibre_scotland.csv`\n",
    "\n",
    "</div>\n",
    "\n",
    "Read the raw CityFibre data into a data frame using the pandas `pd.read_csv()` function. This function returns a `DataFrame` object, which can be displayed as a table in Jupyter environments.\n",
    "\n",
    "To call a function from these modules, it is necessary to access the modules by their <code>_module_alias_</code> (or <code>*module_name*</code> if an alias wasn't defined).\n",
    "\n",
    "In this case, to use `read_csv()` from `pd`, it is necessary to write <code>pd.read\\_csv(_file\\_path_)</code>.\n",
    "\n",
    "It is good practice to give the output data frame a clear, descriptive name e.g. `cf_raw`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_raw = pd.read_csv(path.join(input_file_path, \"**myinputfile**.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up column names\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Use the `DataFrame.clean_names(case_type='snake')` function from the `janitor` package to tidy up the columns names so that it is easier to work with.\n",
    "    \n",
    "This will name the column names all lowercase and put an `_` between any multiple words\n",
    "\n",
    "</div>\n",
    "\n",
    "Apply the `DataFrame.clean_names(case_type='snake')` to the raw dataset.\n",
    "\n",
    "Give the output a new name (eg `cf_input`)  as it has how been altered from the raw input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_input = **inputdataset**.clean_names(case_type='snake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understand the CityFibre dataset\n",
    "\n",
    "### Visual Inspection\n",
    "\n",
    "The first thing to do when getting a dataset is to have a look at it.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Type the name of the dataset in a code cell and run it, jupyter will try to display it in a table format. The displayed table will be shown collapsed if the dataset contains very large data.\n",
    "\n",
    "Use the `DataFrame.head(n)` method objects to get the first `n` rows of a dataset. Althought the table will still get collapsed When `n` gets large. \n",
    "    \n",
    "Use the [<code>pd.set_option(*option*, *value*)</code>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.set_option.html) function to change the dimension of tables displayed in a jupyter notebook:\n",
    "\n",
    "```python\n",
    "# sets displayed tables' max rows to 100\n",
    "pd.set_option('display.max_rows', 100)\n",
    "# sets displayed tables' max columns to 100\n",
    "pd.set_option('display.max_columns', 30)\n",
    "# sets the width of displayed tables (in pixel) to avoid wrapping\n",
    "pd.set_option('display.width', 1000)\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_input # run this cell to see how this dataset looks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `head(10)` to look at the first 10 rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset**.head(**numberofrows**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size, shape and format\n",
    "\n",
    "The next step is to understand the full size and shape of the dataset.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "* The `DataFrame.shape` attribute return a tuple that represents the dimensions of the dataset, with `DataFrame.shape[0]` and `DataFrame.shape[1]` refers to the count of rows and count of columns repectively.\n",
    "* The `DataFrame.describe()` is a very useful function that generates descriptive statistics of the data frame.\n",
    "* The [`DataFrame.dtypes`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dtypes.html#pandas.DataFrame.dtypes) attribute returns the numpy data types (known as [dtype](https://numpy.org/doc/stable/reference/arrays.dtypes.html)) of data of each column in the dataset\n",
    "</div>\n",
    "\n",
    "Use the `DataFrame.shape` to show the dataset dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset**.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DataFrame.describe()` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset**.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DataFrame.dtypes` instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset**.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all these different views it can be seen that the dataset has 66438 rows and 4 columns of `object` data. \n",
    "\n",
    "The data type of `object` is not very useful.\n",
    "\n",
    "A simple way to help us guess what that object might be is to use [`DataFrame.convert_dtypes()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.convert_dtypes.html#pandas.DataFrame.convert_dtypes) method to auto recognise data types and return a new dataframe with best-guessed types annotated.\n",
    "\n",
    "Experiment with `DataFrame.convert_dtypes()` to return a guess at more specific dtype information.\n",
    "\n",
    "Does the returned data frame has more specific dtype information? (tip: test it using `DataFrame.dtypes`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset**.convert_dtypes().dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four columns are returned as `string` type. Guesses can however go wrong. \n",
    "\n",
    "Use the default `object` dtypes for now and will only explicitly specify dtypes when they are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding of each column**\n",
    "\n",
    "* `city` is the name of the city that the data in each row belongs to\n",
    "* `city_code` is a 3-letter code for each city\n",
    "* `postcode` is the postcode for each line of data\n",
    "* `node` is CityFibre's identifier for each part of the city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows does the input CityFibre file have?\n",
    "\n",
    "How many columns?\n",
    "\n",
    "What information is represented in each column?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "The next step is to identify whether any of the columns contain missing values.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "There are two types of missing values in python:\n",
    "\n",
    "* None - Represents no value. This is the more common method for representing missing data.\n",
    "* NaN - Not a Number. This is returned for numerical values that cannot be calculated.\n",
    "\n",
    "The utility function `pd.isna()` takes a data frame or a series of data and will return a new `DataFrame` with each cell filled with a boolean to indicate whether the given data contains any missing value (None or NaN). If the cell contains missing value, it will be replaced by `True`, otherwise the cell value will be replaced by `False`.\n",
    "\n",
    "This allows us to count missing values by using `DataFrame.sum()`, which adds up all the values in a single column, where `False` will be treated as 0 and `True` will be treated as 1.\n",
    "\n",
    "</div>\n",
    "\n",
    "Calculate number of missing values in each column of the `cf_input` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.isna(**inputdataset**).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Do any of the columns contain missing values?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique values\n",
    "\n",
    "The next step is to identify unique values present in each column.\n",
    "\n",
    "First, we need to be able to select a column. To access the content of a column we uses the subscript syntax <code>*data\\_frame*\\[*column\\_name*\\]</code> where _column\\_name_ is a string. For example, to select the column named, type\n",
    "\n",
    "```python\n",
    "cf_input[\"city\"]\n",
    "```\n",
    "\n",
    "This returns a [`Series`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object, that can be treated as an array-like structure which contains a series of data.\n",
    "\n",
    "Use [<code>pd.unique(*series\\_object*)</code>](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.unique.html) or [`Series.unique()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.unique.html) to calculate the number of distinct values in each column of the `cf_input` dataset, and print them using python's `print()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pd.unique()` on each dataset column and `print()` out the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(pd.unique(**inputdataset**[\"**var1**\"]))\n",
    "print(pd.unique(**inputdataset**[\"**var2**\"]))\n",
    "\n",
    "#Do this for all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Series.unique` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(**inputdataset**[\"**var1**\"].unique())\n",
    "print(**inputdataset**[\"**var2**\"].unique())\n",
    "\n",
    "#Do this for all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "Do they return the same result?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resuling array is too long to be displayed in this notebook. \n",
    "\n",
    "It is possible to find out the element count in these resultant arrays using [`Series.size`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.size.html).\n",
    "\n",
    "Print the element counts using `print()` for columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(pd.unique(**inputdataset**[\"**var1**\"]).size)\n",
    "print(pd.unique(**inputdataset**[\"**var2**\"]).size)\n",
    "\n",
    "#Do this for all variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many different cities are represented in the data?\n",
    "\n",
    "How many different nodes are there?\n",
    "\n",
    "What does the difference between the number of postcodes and the size of the dataset imply?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that [`DataFrame.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html) or [`Series.describe()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.describe.html) already show how many unique values are present. That is true only for object data. For numeric data the results will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Relationships\n",
    "\n",
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What are the 7 distinct values for `city` and `city_code`? \n",
    "    \n",
    "Is there a mapping or relationship between them?\n",
    "\n",
    "</div>\n",
    "\n",
    "Use the [`pd.crosstab()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) function to compare the `city` and `city_code` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(**inputdataset**['**var1**'], **inputdataset**['**var2**'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What does this say about the relationship between `city` and `city_code`?\n",
    "\n",
    "Does the CityFibre dataset cover the whole of Scotland?\n",
    "\n",
    "Is there a relationship between `city` and the `postcode` area?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Postcode area is the first one or two letters of the postcode string before the first numeric digit. More information about postcodes can be found [here](https://en.wikipedia.org/wiki/Postcodes_in_the_United_Kingdom).\n",
    "\n",
    "This will need to be separated out from the postcode into an additional field.\n",
    "\n",
    "A Regular Expression pattern can be used to extract the area code. The pattern used here starts at the beginning of the string, and matches any capital letter 0 or 1 times. A useful tool to test patterns can be found here: https://regexr.com/\n",
    "\n",
    "```python\n",
    "area_pattern = r\"^[A-Z][A-Z]?\"\n",
    "```\n",
    "\n",
    "Add `r` before the string to mark it as a raw string, so python will ignore all escape codes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a new field means it is necessary to create a new column (otherwise known as a `Series` object). \n",
    "\n",
    "There are many ways to create a column based on another column's content. The one we used here is [`Series.str.extract()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html#pandas.Series.str.extract), since it exactly matches our purposes without introducing other concepts.\n",
    "\n",
    "To set the content for the new column, use python assignment syntax:\n",
    "\n",
    "```python\n",
    "df[\"column_name\"] = series\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "area_pattern = r\"(^[A-Z][A-Z]?)\"\n",
    "\n",
    "**inputdataset**[\"pc_area\"] = **inputdataset**['postcode'].str.extract(area_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the [`pd.crosstab()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.crosstab.html) function as above to compare `pc_area` with `city_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(**code to make crosstab work**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Does this highlight any issues?\n",
    "\n",
    "What is going on with postcodes in Glasgow and Renfrewshire?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read in supporting datasets\n",
    "\n",
    "<div class=\"alert alter-block alert-success\">\n",
    "\n",
    "There are a number of supporting datasets required for this analysis that can append additional information to the core CityFibre dataset.\n",
    "Most of these datasets come with supporting information, so it is recommended to review the online information and data dictionaries alongside reading in the files.\n",
    "\n",
    "These are:\n",
    "\n",
    "1. The superfast broadband scheme voucher scheme\n",
    "  + This provides postcode-level information about the voucher scheme available to help all properties to access superfast broadband\n",
    "\n",
    "2. The Scottish Index of Multiple Deprivation 2020 indicators, indices and shapefiles\n",
    "  + This is a tool for understanding relative deprivation of areas across Scotland\n",
    "  + The file includes a relative level of deprivation for each data_zone\n",
    "  + The file includes the raw data indicators used to calculated the create the overall level\n",
    "  + The file contains shapefiles the can be used to define the boundaries of the data_zone on a map\n",
    "\n",
    "3. Scottish postcode data, data_zones and shapefiles\n",
    "  + All postcodes in Scotland are split into two files: small user and large user. The large user postcodes cover single addresses receiving >1000 items per day. These are mainly business addresses. The small user postcodes cover on average 15 addresses and the shapefiles highlight the boundaries of these postcodes.\n",
    "  + https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/spd-datadictionary-2020-2.pdf\n",
    "  + https://www.nrscotland.gov.uk/files/geography/Products/postcode-bkgrd-info.pdf\n",
    "  + SIMD mapping between data_zones and postcodes can also be extracted from these files\n",
    "  + data_zones are areas larger than postcodes that are used to aggregate the census information to neighbourhood statistics. Each postcode can be mapped to its surrounding data_zone.\n",
    "  + https://www2.gov.scot/Topics/Statistics/sns/SNSRef/SNSPapDatZon\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scottish Broadband Voucher Scheme\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "https://www.scotlandsuperfast.com/\n",
    "\n",
    "https://www.scotlandsuperfast.com/how-can-i-get-it/voucher-scheme/\n",
    "\n",
    "Scotland has a commitment to ensure every address in Scotland has access to a superfast broadband connection by the end of 2021. There is currently a voucher scheme in place for addresses where the current connection speed is less than 30Mbps. There are two types of voucher, £5000 for the main voucher scheme (MVS) and £400 for an interim scheme to help properties where the main rollout is in plan, but not until after 2021.\n",
    "\n",
    "The `scheme_references.csv` has data provided by CityFibre at postcode level about the possible level of voucher available.\n",
    "\n",
    "</div>\n",
    "\n",
    "Read in the `scheme_references.csv` dataset and clean up names on input using \"snake\" case.\n",
    "\n",
    "Call the output dataset `sbvs_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sbvs_input = pd.read_csv(path.join(input_file_path, \"**inputfile**\")).clean_names(case_type='**case**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMD 2020\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "https://www.gov.scot/publications/scottish-index-multiple-deprivation-2020\n",
    "\n",
    "The dataset has an identifier called `data_zone` which is larger than a postcode. More information on Data Zones can be found [here](https://www.isdscotland.org/Products-and-Services/GPD-Support/Geography/). Each Data Zone contains around 500-1000 people and are the smallest level for summarising the census information.\n",
    "\n",
    "</div>\n",
    "\n",
    "Before starting it is necessary to access the SIMD data which can be found here:\n",
    "http://sedsh127.sedsh.gov.uk/Atom_data/ScotGov/ZippedShapefiles/SG_SIMD_2020.zip\n",
    "\n",
    "Put this folder as a subdirectory in the `data` folder.\n",
    "\n",
    "[`gpd.read_file()`](https://geopandas.org/docs/reference/api/geopandas.read_file.html?highlight=read_file) is a function which returns a [`GeoDataFrame`](https://geopandas.org/docs/reference/api/geopandas.GeoDataFrame.html#geopandas.GeoDataFrame), which is a special type of data frame with the boundary geometry data attached.\n",
    "\n",
    "Read in the  \".shp\" dataset from the `SG_SIMD_2020` folder folder using the `gpd.read_file()` function from the Geopandas package.\n",
    "\n",
    "Again clean up the names on input using `clean_names()`.\n",
    "\n",
    "Call the output dataset `simd_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simd_input = gpd.read_file(path.join(input_file_path, \"SG_SIMD_2020/SG_SIMD_2020.shp\")).clean_names(case_type=\"snake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scottish postcode data\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "https://www.nrscotland.gov.uk/statistics-and-data/geography/our-products/scottish-postcode-directory/2020-2\n",
    "\n",
    "Background information on postcodes can be found [here](https://www.nrscotland.gov.uk/files/geography/Products/postcode-bkgrd-info.pdf)\n",
    "Postcodes are split into two types: small and large.\n",
    "\n",
    "Small user postcodes are based on one or more addresses. There are on average 15 delivery points in a single postcode with a boundary polygon around each postcode. The polygons cover the whole of Scotland's land surface.\n",
    "\n",
    "Large user postcodes are allocated to single addresses that receive in excess of 1000 items of mail per day. There are no boundaries, but each large user is linked to its nearest small user postcode.\n",
    "\n",
    "</div>\n",
    "\n",
    "Before starting it is necessary to access the postcode information.\n",
    "This can be the latest version if preferred.\n",
    "\n",
    "Download the postcode unit boundaries, unzip and save the whole directory into the `data` directory.\n",
    "https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/PC_Cut_20_2.zip\n",
    "\n",
    "Download the postcode indexes and save the small and large user files into the same directory\n",
    "https://www.nrscotland.gov.uk/files//statistics/geography/2020-2/SPD_PostcodeIndex_Cut_20_2_CSV.zip\n",
    "\n",
    "Using `pd.read_csv()` read in the `SmallUser.csv` file. Cleaning names on input, call this `pc_small_input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**smallfile** = **readfunction**(path.join(input_file_path, \"PC_Cut_20_2/SmallUser.csv\")).clean_names(case_type='snake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `LargeUser.csv` file, cleaning names on input, call this `pc_large_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**largefile** = **readfunction**(path.join(input_file_path, \"PC_Cut_20_2/LargeUser.csv\")).clean_names(case_type='snake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the supporting datasets have now being read in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding supporting datasets\n",
    "It is now necessary to understand the supporting datasets in more detail using a similar approach to the CityFibre dataset.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The steps required for each dataset are:\n",
    "\n",
    "* Visual inspection\n",
    "* Size, shape and format\n",
    "* Missing values\n",
    "* Unique values\n",
    "* Relationships between columns\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scottish Broadband Voucher Scheme\n",
    "\n",
    "#### Visual inspection\n",
    "Use `head()` to have a quick look at the dataset to see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**sbvsfile**.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains two columns. One with postcode and one containing the scheme reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size, shape and format\n",
    "Use `DataFrame.shape` and `DataFrame.describe()` and `DataFrame.dtypes` to identify the number of rows, columns and the format of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**sbvsfile**.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**sbvsfile**.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**sbvsfile**.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows are in the dataset?\n",
    "\n",
    "How many columns?\n",
    "\n",
    "What type of data is present?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.isna(**sbvsfile**).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there any missing values present?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values\n",
    "How many of the values are unique (using `Series.unique()`)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(**sbvsfile**[\"**var1**\"].unique().size)\n",
    "print(**sbvsfile**[\"**var2**\"].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many different values of postcode are present?\n",
    "\n",
    "Do you expect this file to contain duplicates?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Series.value_counts()` to get occurence counts of each different value of `scheme_references`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**sbvsfile**[\"scheme_references\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many different values of `scheme_reference` are there?\n",
    "\n",
    "What do they represent?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIMD 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual inspection\n",
    "Use `DataFrame.head()` to have a quick look at the dataset to see what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**simdfile**.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SIMD file contains a lot of columns including some geometry information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size, shape and format\n",
    "Use `DataFrame.shape` and `DataFrame.describe()` and `DataFrame.dtypes` to identify the number of rows, columns and the format of each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**simdfile**.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**simdfile**.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**simdfile**.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows are in the dataset?\n",
    "\n",
    "How many columns?\n",
    "\n",
    "What type of data is present?\n",
    "\n",
    "Are there any obvious data quality issues on visual inspection that will need to be addressed?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "Look at the missing values for the key field which is `data_zone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use isna() and sum() to look for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there any missing values for the `data_zone` variable?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values\n",
    "Look at the unique values for the `data_zone` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use unique() and size to look at the number of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there duplicates in the data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scottish postcode data\n",
    "\n",
    "#### Visual inspection\n",
    "Use `DataFrame.head()` to have a quick look at both the small and large postcode files to see what they contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use head() to have a quick look at the small and large user postcode files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file contains similar information, but there are differences between the two in terms of the columns and level of information.\n",
    "`pc_small_user` also contains some census information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size, shape and format\n",
    "Use `DataFrame.shape`, `DataFrame.describe()` and `DataFrame.dtypes` to identify the number of rows, columns and the format of each postcode file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small user shape(), describe() and format\n",
    "\n",
    "# large user shape(), describe() and format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows are in each file?\n",
    "\n",
    "How many columns?\n",
    "\n",
    "Are the columns the same?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The small and large postcode files have different number of columns and therefore contain slightly different information.\n",
    "\n",
    "It would be useful understand which columns occur in both datasets and which do not.\n",
    "\n",
    "The `DataFrame.columns` function can be used to extract the column names into a separate array-like structure (`Index`) for comparison.\n",
    "\n",
    "Since many Numpy functions can process array-like data structure, we can then use [Set operation functions from Numpy](https://numpy.org/doc/stable/reference/routines.set.html) `np.intersect1d()` and `np.setdiff1d()` to compare the two resulting `Index`.\n",
    "\n",
    "Notice that `np.setdiff1d(A, B)` is not the same as `np.setdiff1d(B, A)`\n",
    "\n",
    "</div>\n",
    "\n",
    "Use the `DataFrame.columns` function to extract two array-link structure of column names for the two postcode files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "small_names = **smallpostcodefile**.columns\n",
    "large_names = **largepostcodefile**.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `np.intersect1d()` function to look at the columns in common between the two datasets.\n",
    "\n",
    "Display the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_names_intersected = np.intersect1d(small_names, large_names)\n",
    "pc_names_intersected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_names_intersected.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many columns are in common across both datasets?\n",
    "\n",
    "Will all this information be needed for the analysis?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `np.setdiff1d()` function twice to look at the columns that are not in common between the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.setdiff1d(small_names, large_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.setdiff1d(large_names, small_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What information does the small user file contain that is not in the large user file?\n",
    "\n",
    "What information does the large user file contain that is not in the small user file?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "\n",
    "Look at missing values for both postcode files for the key field which is `postcode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small user - sum the number of missing values of postcode\n",
    "\n",
    "# large user - sum the number of missing values of postcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there any missing values?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique values\n",
    "Look at distinct values of `postcode` for both files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small user - number of unique values\n",
    "\n",
    "# large user - number of unique values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are any of the postcodes duplicated?\n",
    "\n",
    "Why might this be?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 3 - Postcode data cleaning\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 3 - Postcode dataset cleaning**\n",
    "\n",
    "This next lesson goes through the steps required to clean the postcode dataset. The output cleaned dataset is then used for postcode validation and mapping between postcodes and datazones.\n",
    "\n",
    "This involves:\n",
    "\n",
    "* Subsetting the postcode datasets to the columns of interest\n",
    "* Joining the small and large datasets together\n",
    "* Investigating the reason behind duplicates and removing them\n",
    "* Adding descriptions to the Urban Rural classification\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the required columns\n",
    "\n",
    "The postcode datasets both contain over 50 columns, not all of these are required for the further analysis.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The required columns in the postcode files are:\n",
    "\n",
    "+ postcode\n",
    "+ postcode_district\n",
    "+ postcode_sector\n",
    "+ date_of_introduction\n",
    "+ date_of_deletion\n",
    "+ grid_reference_easting\n",
    "+ grid_reference_northing\n",
    "+ latitude\n",
    "+ longitude\n",
    "+ split_indicator\n",
    "+ data_zone2011_code\n",
    "+ urban_rural8_fold2016_code\n",
    "\n",
    "This gives the geographical information, the current validity of the postcode, the link to the `data_zone` and the `urban_rural` code which highlights the type of location the postcode is in.\n",
    "\n",
    "The `urban_rural` information has been requested to be used by CityFibre, as they wish to focus on the more rural postcodes with existing poor connections.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame[['required_col1', 'required_col2', ...]]` subset the small and large postcode files to the set of required columns.\n",
    "\n",
    "In addition, using <code>DataFrame.assign(_new\\_col\\_name_ = _col\\_value_)</code> create an additional field to identify the postcode type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "required_column_names = [\n",
    "  \"postcode\" ,\"postcode_district\" ,\"postcode_sector\" ,\n",
    "  \"date_of_introduction\" ,\"date_of_deletion\" ,\n",
    "  \"grid_reference_easting\" ,\"grid_reference_northing\" ,\n",
    "  \"latitude\" ,\"longitude\" ,\"split_indicator\" ,\n",
    "  \"data_zone2011_code\" ,\"urban_rural8_fold2016_code\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_small_subset = pc_small_input[required_column_names]\n",
    "pc_small_subset = pc_small_subset.assign(**newvar**=\"small\")\n",
    "pc_small_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_large_subset = pc_large_input[required_column_names]\n",
    "pc_large_subset = pc_large_subset.assign(**newvar**=\"large\")\n",
    "pc_large_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join small and large user datasets together\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The small and large datasets contain the same columns, so the required join is an **append**, whereby one dataset is added to the bottom of the other.\n",
    "\n",
    "The `DataFrame.append()` function is used to append datasets.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a combined dataset by appending both the small and large postcode files together.\n",
    "\n",
    "Name this resulting output file `pc_combined`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_combined = **dataset1**.append(**dataset2**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.sort_values()` to sort alphabetically by `postcode`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**inputdataset** = **inputdataset**.sort_values('**sortingvar**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the resulting sorted file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate and remove duplicates\n",
    "\n",
    "It was identified in **Lesson 2 - Data Understanding** that the postcodes were not all unique. This requires further investigation.\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The investigation starts by identifying the duplicate postcodes.\n",
    "\n",
    "A detailed look at an example duplicate should be able to highlight the issue.\n",
    "\n",
    "The function `DataFrame.value_counts()` counts the unique values of a variable\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `DataFrame.value_counts()` function to group the combined postcode file by postcode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**check** = **inputdataset**.value_counts('**countvar**')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort this output check by the descending value of count.\n",
    "Set `DataFrame.sort_values()` method's `ascending` argument to `False` to reorder the output check with the most duplicated postcodes at the top.\n",
    "Use `DataFrame.head()` to print out the top 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**check** = **check**.sort_values(ascending=False)\n",
    "**check**.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Which postcode(s) has the most duplicates?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the combined postcode file to only the rows with the most duplicated postcode.\n",
    "\n",
    "Print out the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**maxpostcode** = **inputdataset**[**inputdataset**[\"postcode\"] == \"**mostduplicatedpc**\"]\n",
    "**maxpostcode**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Why has this postcode been duplicated multiple times since 1980?\n",
    "\n",
    "How can a \"live\" postcode be identified in the data?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retain only live postcodes\n",
    "\n",
    "Use boolean indexing and `Series.isna()` to create a postcode file containing only live postcodes with a missing `date_of_deletion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**live** = **inputdataset**[**inputdataset**[\"**missingvar**\"].isna()]\n",
    "**live**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many \"live\" postcodes are there?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally check there are no duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check for duplicates by counting unique postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there any duplicates remaining?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urban Rural classification reference data\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The urban rural 8-fold classification code is a number between 1 and 8. However that does not provide information on what the code means.\n",
    "\n",
    "It is useful to create a reference (lookup) dataset that maps the code to its description.\n",
    "\n",
    "The definitions for urban rural 8-fold classification can be found [here](https://www.gov.scot/publications/scottish-government-urban-rural-classification-2016/pages/2/)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list containing the codes 1 to 8 called `urban_rural8_fold2016_code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**codelist** = list(range(1, 9))\n",
    "**codelist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list containing the names of the codes called `urban_rural8_fold2016_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**namelist** = [\"Large Urban Areas\",\n",
    "                            \"Other Urban Areas\",\n",
    "                            \"Accessible Small Towns\",\n",
    "                            \"Remote Small Towns\",\n",
    "                            \"Very Remote Small Towns\",\n",
    "                            \"Accessible Rural Areas\",\n",
    "                            \"Remote Rural Areas\",\n",
    "                            \"Very Remote Rural Areas\"]\n",
    "**namelist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new DataFrame by passing a dictionary that has the two lists in a data frame. The dictionary keys will become the column names. \n",
    "\n",
    "Use `\"urban_rural8_fold2016_code\"` and `\"urban_rural8_fold2016_name\"` as their keys respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ur_ref = pd.DataFrame(data = {\n",
    "    \"urban_rural8_fold2016_code\": urban_rural8_fold2016_code,\n",
    "    \"urban_rural8_fold2016_name\": urban_rural8_fold2016_name\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out this reference dataset to make sure the codes match up correctly with the names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ur_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge this reference dataset into the live postcode data file, joining on the code\n",
    "Use a `pd.merge()` to perform left join by passing `\"left\"` to keyword `how`, and set keyword `on` to `urban_rural8_fold2016_code` to use it as the join key.\n",
    "\n",
    "Review the output dataset to ensure the join has worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_live = pd.merge(pc_live, ur_ref, on=\"urban_rural8_fold2016_code\", how=\"left\")\n",
    "pc_live"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 4 - SIMD data cleaning\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 4 - SIMD dataset cleaning**\n",
    "\n",
    "This next lesson goes through the next step of the required analysis: cleaning the SIMD dataset.\n",
    "\n",
    "The SIMD dataset currently contains numeric values stored as percentages that need to have the \"%\" removed and converted to proper numeric values.\n",
    "\n",
    "The column names are also not intuitive, so may lead to confusion in future analysis. It is therefore necessary to rename them.\n",
    "\n",
    "The steps involved include:\n",
    "\n",
    "* Selecting the required columns\n",
    "* Converting to numeric those columns that are in string format\n",
    "* Renaming all columns to a descriptive variable name\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retain columns of interest\n",
    "\n",
    "Use the `DataFrame.columns` to output the names all the fields in the `simd_input` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**simdfile**.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The problem statement requires the analyst to focus on SIMD, income, employment and broadband access.\n",
    "\n",
    "The names of these fields in the dataset are:\n",
    "\n",
    "* rankv2 - SIMD rank\n",
    "* inc_rate\n",
    "* emp_rate\n",
    "* g_acc_brdbnd\n",
    "\n",
    "Other fields to retain for reference or further analysis are:\n",
    "\n",
    "* data_zone\n",
    "* dz_name\n",
    "* sape2017 - population\n",
    "* wape2017 - working age population\n",
    "\n",
    "Other fields may also be retained. The detail of each field can be found in the SIMD technical notes:\n",
    "\n",
    "[SIMD technical notes](https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/09/simd-2020-technical-notes/documents/simd-2020-technical-notes/simd-2020-technical-notes/govscot%3Adocument/SIMD%2B2020%2Btechnical%2Bnotes.pdf)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame[['required_col1', 'required_col2', ...]]` to filter the required fields.\n",
    "Name the output dataset `simd_retain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**outputdataset** = **inputdataset**[[**list of retained fields separated by commmas**]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Convert string columns to numerics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.head()` to review the first few rows of the retained dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# quick look at the output dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.dtypes` to review the format of the remaining dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review the formats of the output dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Which of the remaining variables should be numeric, but are currently stored as character values?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "To fix these columns is a two-step process:\n",
    "\n",
    "* remove the %'s\n",
    "* convert the columns to numeric\n",
    "\n",
    "The `Series.str.strip(\"%\")` function can be used to remove the `%`s\n",
    "\n",
    "The <code>pd.to\\_numeric(_varname_)</code> can then be used to change the variable type.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Series.str.strip()` and `pd.to_numeric()` within `DataFrame.assign()` statement for each of the columns that require fixing. Write this out to a new cleaned dataset `simd_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**cleanoutput** = **inputdataset**.assign(**var1** = pd.to_numeric(**inputdataset**[\"**var1**\"].str.strip(\"%\")),\n",
    "                                **var2** = pd.to_numeric(**inputdataset**[\"**var2**\"].str.strip(\"%\")),\n",
    "                                **var3** = pd.to_numeric(**inputdataset**[\"**var3**\"].str.strip(\"%\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Rename columns to be more descriptive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.columns` to write out the column names on the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**cleanoutput**.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The column names are not very intuitive.\n",
    "\n",
    "By referring to the Technical Notes it be seen that:\n",
    "\n",
    "* sape2017 is total population within the datazone\n",
    "* wape2017 is the working age population within the datazone\n",
    "* rankv2 is the rank across the whole of Scotland for that datazone. Low ranks are the most deprived, high ranks are the least deprived.\n",
    "* quintilev2 splits the ranks up into 5 groups containing 20% each. quintile 1 is the most deprived. quintile 5 is the least deprived.\n",
    "* decilev2 splits the ranks up into 5 groups containing 10% each. Similar to quintiles, 1 is the most deprived and 10 is the least deprived.\n",
    "* inc_rate is the percentage of adults in the datazone receiving income support.\n",
    "* emp_rate is the percentage of adults in the datazone receiving employment support\n",
    "* g_acc_brdbnd is the percentage of premises that **do not** have access to superfast broadband of 30Mb/s\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all the columns, use `DataFrame.rename()` to give them a variables a name that is more descriptive.\n",
    "\n",
    "Remember to continue to use *snake_case* words to be consistent with the existing naming convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**cleanoutput** = **cleanoutput**.rename(columns={\n",
    "    \"**oldname1**\": \"**newname1**\",\n",
    "    \"**oldname2**\": \"**newname2**\",\n",
    "    #continue to rename all the columns\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.head()` to review the first few rows of the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# quick look at the cleaned output file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 5 - CityFibre data validity and cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 5 - CityFibre data validity and cleaning**\n",
    "\n",
    "This lesson goes through the next step of the CityFibre data analysis problem: cleaning and validating the CityFibre provided dataset.\n",
    "\n",
    "Some initial problems have already been hinted at that require further investigation:\n",
    "\n",
    "* Duplicate postcodes\n",
    "* Some Glasgow postcodes have been identified in Renfrewshire and vice versa\n",
    "* Retaining only \"live\" postcodes\n",
    "\n",
    "Additional issues may also be identified as the dataset is validated\n",
    "\n",
    "The steps involved include:\n",
    "\n",
    "* Dealing with duplicate rows\n",
    "* Dealing with duplicate postcodes\n",
    "* Retaining only live postcodes\n",
    "* Reformatting the postcode to ensure files can be joined correctly\n",
    "* Checking validity\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dealing with duplicate rows\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The `DataFrame.drop_duplicates()` method will only **deduplicate** exact rows.\n",
    "\n",
    "</div>\n",
    "\n",
    "Apply `DataFrame.drop_duplicates()` to the `cf_input` dataset and rename the output as `cf_dedupe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**dedupedataset** = **inputdataset**.drop_duplicates()\n",
    "**dedupedataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows were there in the input dataset?\n",
    "\n",
    "How many rows are in the deduped dataset?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicate postcodes\n",
    "\n",
    "Use `Series.unique().size` to check for the remaining number of unique postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique postcodes remain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are there still duplicates remaining?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Series.value_counts()` to get a frequency table for each postcode, then perform left join on `cf_dedupe` using `DataFrame.join()` on its `postcode` field, give the new column a suffix `_count` using keyword `rsuffix` of `DataFrame.join()`. \n",
    "\n",
    "The new data frame will then have a `postcode_count` variable. \n",
    "\n",
    "Store the joined data frame to a new variable named `cf_dupes`.\n",
    "\n",
    "Filter `cf_dupes` value using boolean indexing and assign it back to cf_dupes. Sort by `postcode` field using `DataFrame.sort_values()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**duplicates** = cf_dedupe.join(**dedupedataset**[\"postcode\"].value_counts(),\n",
    "                          on=\"postcode\", rsuffix=\"_count\")\n",
    "**duplicates** = **duplicates**[**duplicates**[\"postcode_count\"] > 1]\n",
    "**duplicates** = **duplicates**.sort_values(\"postcode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print()` the resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Print out the dataset of duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "There are two remaining issues causing duplicates?\n",
    "\n",
    "What are they both?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 1 - Glasgow/Refrewshire postcodes\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Renfrewshire postcodes should start with PA and Glasgow postcodes should start with G\n",
    "Delete any that don't follow this pattern in the `cf_dedupe` dataset\n",
    "\n",
    "Create a boolean field that confirms whether area code matches the city\n",
    "Then filter out those that fail the check.\n",
    "\n",
    "The easiest way to create the check is to merge in a reference dataset with the correct city related to the pc_area and then compare cities.\n",
    "\n",
    "</div>\n",
    "\n",
    "Create a list of all possible values of `pc_area`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_area = [\"AB\", \"DD\", \"EH\", \"G\", \"IV\", \"PA\", \"FK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list with the correct city for that postcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_city = [\"Aberdeen\", \"Dundee\", \"Edinburgh\", \"Glasgow\", \"Inverness\", \"Renfrewshire\", \"Stirling\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a reference data frame containing both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_ref = pd.DataFrame(data = {\n",
    "    'pc_area' : **list1**,\n",
    "    'pc_city' : **list2**\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print()` it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print out the reference data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge into the `cf_dedupe` dataset using a `pd.merge()` keyed on `pc_area`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**checkdataset** = pd.merge(**dedupedataset**, **referencedataset**, how=\"left\", on=\"pc_area\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.assign` to create a check field `pc_check` that tests whether the `city == pc_city`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**checkdataset** = cf_check.assign(pc_check = **checkdataset**['city'] == **checkdataset**['pc_city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using boolean indexing to identify the rows that fail the check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_fail = **checkdataset**[**checkdataset**[\"pc_check\"] == False]\n",
    "cf_fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is this issue happening all over Scotland?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter these values out by boolean indexing `cf_check[\"pc_check\"] == True`, calling the resulting dataset `cf_dedupe2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_dedupe2 = **checkdataset**[**checkdataset**[\"pc_check\"] == True]\n",
    "cf_dedupe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue 2 - Multiple CityFibre nodes\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The final issue is related to the level of the file.\n",
    "\n",
    "The CF input dataset is not actually at postcode level, it is at postcode and node level.\n",
    "\n",
    "It is necessary to reduce this dataset to postcode level by removing the node field and deduping.\n",
    "\n",
    "</div>\n",
    "\n",
    "Create a new postcode level file.\n",
    "\n",
    "Drop the `node` field using `DataFrame.drop(columns=node)`.\n",
    "\n",
    "Only keep distinct rows using `drop_duplicates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_pc_level = cf_dedupe2.drop(columns=\"node\").drop_duplicates()\n",
    "cf_pc_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a final check to ensure all the remaining postcodes are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check the number of unique postcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Have all duplicates been removed?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retain only \"live\" postcodes\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Now that the postcode file has been prepared it is possible to check the validity of the postcodes in the CityFibre-provided dataset.\n",
    "\n",
    "By performing an *outer* join using `pd.merge()` and passing in a `True` the keyword `indicator`. It will be possible to return a data frame with a `_merge` field, which identifies the set of matching and non_matching items.\n",
    "\n",
    "It will then be possible to see which ones don't match a live postcode.\n",
    "\n",
    "</div>\n",
    "\n",
    "Create a 'non_match' dataset using an `pd.merge()` keyed on `\"postcode\"` and `\"postcode\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_match = pd.merge(cf_pc_level, **livepostcodes**, left_on=\"postcode\", right_on=\"postcode\", how=\"outer\", indicator=True)\n",
    "non_match = non_match[non_match[\"_merge\"] == \"left_only\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many non-matching postcodes have been identified?\n",
    "\n",
    "Does this feel like a realistic number?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the set of non_matching postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "The large number of non-matches have highlighted an additional issue. What has caused it?\n",
    "\n",
    "Look at the formatting of the join variable. It will only join if the matches are exact.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postcode format\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "All the postcodes need to be consistently formatted.\n",
    "\n",
    "The most reliable consistent postcode format is for all spaces to be removed and is all in uppercase.\n",
    "This new \"postcode\" field will need to be created in all the supporting files to support joining between all the different working datasets.\n",
    "\n",
    "</div>\n",
    "\n",
    "Create an additional `postcode2` field that has had all the spaces removed and in uppercase in the `cf_pc_level` dataset using `DataFrame.assign()`.\n",
    "Use `Series.str.upper()` for uppercase and `Series.str.replace(\" \", \"\")` to remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**outputdataset** = **inputdataset**.assign(postcode2 = **inputdataset**[\"postcode\"].str.upper().str.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.head()` to review the output dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review the output dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create additional `postcode2` fields in the `pc_live` and `sbvs_input` datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**livepostcodes** = **livepostcodes**.assign(# create postcode2 variable)\n",
    "\n",
    "**sbvsdataset** = **sbvsdataset**.assign(# create postcode2 variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo validity check\n",
    "\n",
    "Redo the *outer* join check but this time merging on `postcode2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "non_match2 = pd.merge(cf_pc_level, **livepostcodes**, left_on=\"postcode2\", right_on=\"postcode2\", how=\"outer\", indicator=True)\n",
    "non_match2 = non_match2[non_match2[\"_merge\"] == \"left_only\"]\n",
    "non_match2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many non-matches remain?\n",
    "\n",
    "What might be causing these remaining non-matches?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "These remaining non-matches should be removed from the CityFibre dataset as they are no longer valid live postcodes.\n",
    "\n",
    "To do this we need to perform semi join, which means we need to perform inner join first to get intersected `postcode2`s, then boolean indexing rows which has these intersected `postcode2` from the *left table*.\n",
    "\n",
    "</div>\n",
    "\n",
    "Perform inner join with `pd.merge()`, then boolean indexing `cf_pc_level` with `postcode2` from the merged table to create a table of valid CityFibre postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_pc_valid = pd.merge(cf_pc_level, **livepostcodes**,\n",
    "                       left_on=\"postcode2\", right_on=\"postcode2\",\n",
    "                       how=\"inner\")\n",
    "in_both = cf_pc_level[\"postcode2\"].isin(cf_pc_valid[\"postcode2\"])\n",
    "cf_pc_valid = cf_pc_level[in_both]\n",
    "cf_pc_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "How many rows remain?\n",
    "\n",
    "Has this join method performed as expected?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives a clean set of starting postcodes for the CityFibre analysis - `cf_pc_valid`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 6 - Creating output datasets for analysis\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 6 - Creating output analysis datasets**\n",
    "\n",
    "The final step in the data preparation is to create the datasets that will be used within the analysis itself.\n",
    "    \n",
    "There are 2 key files to create:\n",
    "    \n",
    "* a postcode level file\n",
    "* a datazone level file\n",
    "\n",
    "The preparation of these files will involve merging in additional information at either postcode or datazone level. All the postcode level information will use the new postcode2 field, to avoid any merging issues.\n",
    "    \n",
    "The datazones should only be those of interest for CityFibre.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating postcode level analysis file\n",
    "\n",
    "### Add scheme_references\n",
    "Need to merge in the scheme_references.\n",
    "\n",
    "Start by creating the input table for merging.\n",
    "Create a `sbvs_merge` dataset containing `postcode2` and `scheme_reference` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sbvs_merge = **sbvsdataset**.drop(columns=\"postcode\")\n",
    "sbvs_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a left join using `pd.merge()` with `postcode2` as their keys, merge in the `scheme_references`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "**outputdataset** = pd.merge(cf_pc_valid, sbvs_merge, how=\"left\", on=\"**joinvar**\")\n",
    "**outputdataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the required postcode information\n",
    "\n",
    "Start by creating the input table for merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `DataFrame.columns` to view the columns in `pc_live`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# view the columns in `pc_live`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The required fields for merging are:\n",
    "\n",
    "* grid_reference_easting \n",
    "* grid_reference_northing \n",
    "* latitude \n",
    "* longitude\n",
    "* pctype\n",
    "* postcode2\n",
    "\n",
    "In addition the following fields are required and should be renamed\n",
    "* data_zone2011_code - data_zone\n",
    "* urban_rural8_fold2016_code - ur_class\n",
    "* urban_rural8_fold2016_name - ur_name\n",
    "\n",
    "</div>\n",
    "\n",
    "Using `df[['col1', 'col2'...]]` to select the above fields then rename them using `DataFrame.rename()` and store it into `pc_merge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_merge = pc_live[#list of select fields separated by commas]\n",
    "pc_merge = pc_merge.rename(columns={\n",
    "    \"**oldname1**\": \"**newname1**\",\n",
    "    '**oldname2**': \"**newname2**\",\n",
    "    \"**oldname3**\": \"**newname3**\"\n",
    "})\n",
    "pc_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a left join with `pd.merge()` to merge this into the latest CityFibre dataset using `postcode2` as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**outputdataset** = pd.merge(**workingdataset**, pc_merge, how=\"left\", on=\"**joinvar**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add in the SIMD data at postcode level\n",
    "\n",
    "Create an input table for merging from `simd_clean` which contains only the ranks, rates, metrics and the data_zone for merging on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "simd_merge = simd_clean[[\"data_zone\", \"income_support_rate\", \"employment_support_rate\", \"nobroadband_rate\", \"simd5\", \"simd10\", \"simd_rank\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform left join with `pd.merge()` by joining on the `data_zone` to create the final postcode level analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_pc_analysis = pd.merge(**workingdataset**, simd_merge, how=\"left\", on=\"data_zone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.head()` review the final postcode-level analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review the postcode level analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Create data_zone level analysis file\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "For the SIMD data, which is keyed on data_zone, the data_zones of interest are those linked to the live CityFibre postcodes. This is a subset of all the possible data_zones.\n",
    "\n",
    "</div>\n",
    "\n",
    "Select the column `data_zone` and `drop_duplicates()` to create a `Series` just containing the CityFibre data_zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_data_zones = cf_pc_valid3[\"data_zone\"].drop_duplicates()\n",
    "cf_data_zones.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Why are there duplicate data zones?\n",
    "\n",
    "How many datazones in total are linked to the set of valid CityFibre postcodes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge in the cleaned SIMD data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The next step is to extract the set of SIMD data for the subset of datazones identified above.\n",
    "\n",
    "This will require a left join onto the set of datazones.\n",
    "\n",
    "</div>\n",
    "\n",
    "Perform a left join using `pd.merge` to merge in the `simd_clean` dataset by `data_zone`.\n",
    "Name this the datazone level analysis dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_dz_analysis = pd.merge(**datazones**, **simddataset**, how=\"left\", on=\"**joinvar**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.head()` review the final datazone-level analysis dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# review the datazone level analysis dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson 7 - Data Analysis\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 7 - CityFibre data analysis**\n",
    "\n",
    "This lesson goes through the actual analysis of the prepared datasets.\n",
    "\n",
    "The analysis datasets have been prepared. These are:\n",
    "\n",
    "* `cf_pc_analysis`: the CityFibre postcodes at postcode level with supporting postcode level fields\n",
    "* `cf_dz_analysis`: the datazones containing CityFibre postcodes at datazone level and supporting SIMD data\n",
    "\n",
    "At this point it is useful to revisit the problem statement:\n",
    "**Which residents potentially require access to lower cost broadband in our Scottish Cities?**\n",
    "\n",
    "Areas can be prioritised in different ways:\n",
    "\n",
    "* Those with low SIMD ranks - this is at datazone level\n",
    "* Those with currently low rates of access to superfast broadband - this is at datazone level\n",
    "* Those with voucher schemes that will finance the supply of superfast broadband - this is at postcode level\n",
    "* Those in rural locations for which it may be more difficult to supply broadband - this is at postcode level\n",
    "\n",
    "In reality it is a combination of these factors that will identify the areas to focus on.\n",
    "\n",
    "The approach taken is to:\n",
    "\n",
    "* Identify the factors to focus on\n",
    "* Create a score which identifies the the prioritisation of postcodes\n",
    "* Identify a score cut-off for the highest priority postcodes\n",
    "* Experiment with alternative scoring approaches\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate correlations\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "To understand which variables should be included in the score, it is first necessary to understand the relationships between the analysis variables. This gives a feel for which variables are important to the prioritisation.\n",
    "\n",
    "For numeric variables it is possible calculate correlations between them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations for analysis\n",
    "\n",
    "Visualisations help to understand the relationship between variables. One of the most popular visualisation libraries for python is called Seaborn. It's a library that simplifies the use of another library Matplotlib. \n",
    "To use `seaborn` it is necessary to import both `seaborn` and `matplotlib.pyplot`, which was done when setting up the environment.\n",
    "\n",
    "The Seaborn documentation can be found [here](https://seaborn.pydata.org/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income support rate and employment support rate\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The first two variables to compare are `income_support_rate` and `employment_support_rate`.\n",
    "\n",
    "There is an expectation that these variables are related.\n",
    "\n",
    "Using Seaborn, a scatterplot can be created with using the `sns.scatterplot()`.\n",
    "\n",
    "Matplotlib keeps tracking the current working plot, `plt.show()` will show the current working plot in the notebook.\n",
    "</div>\n",
    "\n",
    "Using the Seaborn's `sns.scatterplot()`, plot the relationship between the `income_support_rate` on the x-axis and the `employment_support_rate` on the y-axis from the datazone level analysis file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=**datazonedataset**, x = \"income_support_rate\", y = \"employment_support_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is there a relationship between `income_support_rate` and `employment_support_rate`?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw a best fit line, `sns.regplot()` can be used. \n",
    "\n",
    "The `color` of the best fit line can be changed using the `color` keyword. \n",
    "\n",
    "By default `sns.regplot()` also comes with a scatterplot (through its `scatter` keyword argument). It is possible to also pass additional settings as a dictionary to its `scatter_kws` and `line_kws` keywords. \n",
    "\n",
    "For example, to let the best fit line use `\"orange\"` color, pass in `{\"color\": \"orange\"}` as the `line_kws` keyword argument. The same applies to `scatter_kws` which can be used to customise the scatterplot.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Scatter plot, with best fit line**\n",
    "\n",
    "Matplotlib keeps track of the current working plot behind the scenes. It is possible to plot on top of the existing working plot. Many Seaborn plotting functions i.e. `sns.regplot()` or `sns.scatterplot()`, return an `Axes` object, which contains all the information of a single plot, including axes information (i.e. x-axis and y-axis) and plot title. It is possible to customise axis labels and titles through this `Axes` object. The `Axes` object can be obtained through `plt.gca()` and each notebook cell with share the same `Axes` object. Therefore:\n",
    "    \n",
    "```python\n",
    "sns.scatterplot(...)\n",
    "sns.regplot(..., scatter=False)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "will plot a scatterplot and a best fit line on the same plot.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this relationship again, but this time apply `sns.regplot()`. Use blue as the color of best fit line (through the `line_kws` keyword).\n",
    "\n",
    "Make sure to add a main title, axes titles and any other formatting that allows the key information in the graphic to be emphasised.\n",
    "\n",
    "To specify a color requires to use a format Matplotlib recognises, which are documented [here](https://matplotlib.org/stable/api/colors_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.regplot(data=**datazonedataset**, x = \"income_support_rate\", y = \"employment_support_rate\", line_kws={\"color\":\"orange\"})\n",
    "ax.set_title(\"Relationship between Income Support and Employment Support\")\n",
    "ax.set_xlabel(\"Income Support Rate\")\n",
    "ax.set_ylabel(\"Employment Support Rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is the graph now clearer and easier to interpret?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two variables appear to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the Pearson correlation coefficient using `Series.corr()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inc_emp_corr = **datazonedatset**[\"income_support_rate\"].corr(**datazonedatset**[\"employment_support_rate\"],\n",
    "                                                          method=\"pearson\")\n",
    "inc_emp_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are the variables correlated?\n",
    "\n",
    "What information does the value and sign of the correlation tell us?\n",
    "\n",
    "Do both variables contain similar information?\n",
    "\n",
    "Do we need both variables in future analyses?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income support rate and SIMD rank\n",
    "\n",
    "Use `sns.scatterplot()` to plot the relationship between the `income_support_rate` and the `simd_rank` from the datazone level analysis file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=**datazonedataset**, x = \"**xvar**\", y=\"**yvar**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are the variables linearly related?\n",
    "\n",
    "Is this a positive or negative relationship?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this relationship again, but focus on tidying up the graph for clarity.\n",
    "\n",
    "As the points overplot each other it may be necessary to increase their transparency to view them. Pass a matplotlib recognised color to `sns.scatterplot()`'s `color` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=**datazonedataset**, x = \"**xvar**\", y=\"**yvar**\", color=(0.05,0.4,0.6, 0.3))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Pearson's correlation coefficient between the `income_support_rate` and `simd_rank`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inc_simd_corr = **datazonedataset**[\"**var1**\"].corr(**datazonedataset**[\"**var2**\"],\n",
    "                                         method=\"pearson\")\n",
    "inc_simd_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Are the variables correlated?\n",
    "\n",
    "What information does the value and sign of the correlation tell us?\n",
    "\n",
    "Does the relationship fit with our expectations?\n",
    "\n",
    "Do we need both variables in any future analyses?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Income support rate and broadband access rate\n",
    "\n",
    "Plot the relationship between `income_support_rate` and `nobroadband_rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=**datazonedataset**, x=\"**xvar**\", y=\"**yvar**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is there a strong relationship between these variables?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this relationship again, but focus on tidying up the graph for clarity.\n",
    "Again it may be necessary to deal with the over-plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=**datazonedataset**, x = \"**xvar**\", y=\"**yvar**\", color=(0.05,0.4,0.6, 0.3))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Pearson's correlation coefficient between the `income_support_rate` and `nobroadband_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inc_bb_corr = **datazonedataset**[\"**var1**\"].corr(**datazonedataset**[\"**var2**\"],\n",
    "                                         method=\"pearson\")\n",
    "inc_bb_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is there a strong relationship between these variables?\n",
    "\n",
    "Do we need both variables in any future analyses?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheme references\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "Both `scheme_references` and `ur_class` variables are categorical so it is not possible to carry out a correlation analysis.\n",
    "In this case it is possible to compare the average `nobroadband_rate` in each of the classes and plot this to understand how they are related to each other.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First calculate at the relative frequencies of data in each category at postcode level.\n",
    "\n",
    "Using  `sns.countplot()`, plot the count of `scheme_references`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=**postcodedataset**, x=\"scheme_references\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Is a `scheme_reference` attached to most postcodes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this information is tabulated it is clear to see the difference in counts.\n",
    "\n",
    "Tabulate this information using `Series.value_counts(dropna=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**postcodedataset**[\"**var**\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has ignored all these missing values. To show the count of missing values in the plot, they need to be replaced in the data frame with something else, e.g. string `\"None\"`. \n",
    "\n",
    "This can be achieved through `Series.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**postcodedataset**[\"**var**\"] = **postcodedataset**[\"**var**\"].fillna(\"None\")\n",
    "**postcodedataset**[\"**var**\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, replot with `sns.countplot()`. This time make it more informative and clearer. Make all the bars the same colour. The list of named colours can be found [here](https://matplotlib.org/stable/gallery/color/named_colors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "sns.countplot(data=**postcodedataset**, x=\"**var**\", color = \"**colourname**\")\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that most postcodes are **not** included in the SBVS schemes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average rate of broadband access by scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to review the average rate of access to broadband by `scheme_references`.\n",
    "\n",
    "To calculate this, we need to group `nobroadband_rate` value by their `scheme_references` with <code>DataFrame.groupby(_col_for_grouping_)[_column\\_to\\_put\\_in\\_groups_]</code>, i.e. `DataFrame.groupby(\"scheme_references\")[\"nobroadband_rate\"]`.\n",
    "\n",
    "This returns a `GroupBy` object which contains groups that can be used to perform calculations upon. In our case, we wish to apply `GroupBy.mean()` to calculate the average value of each group.\n",
    "\n",
    "Save the returned value to a variable named `sr_to_nr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sr_to_nr = **postcodedataset**.groupby(\"scheme_references\")[\"nobroadband_rate\"].mean()\n",
    "sr_to_nr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `sr_to_nr`'s `Series.index` are values of `scheme_references` and its `Series.values` are the average values we just calculated. Since we no longer needs to count values, `sns.barplot()` is used instead of `sns.countplot()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the bar chart with `sns.barplot()` and make it more informative and clearer with a single colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.barplot(y = sr_to_nr.values, x = sr_to_nr.index, color = \"**colourname**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What is the difference between the average rate of no access to broadband for each of the values of scheme_reference?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Urban-rural classification\n",
    "\n",
    "First calculate at the relative frequencies of `ur_class` in each category at postcode level using a bar chart. Use `sns.countplot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=**postcodedataset**, x = \"ur_class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabulate this information using `Series.value_counts()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**postcodedataset**[\"**var**\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tidy up this graph so it is clearer and bring in the names of the classes onto the x-axis rather than just the code. Add titles and axis labels for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "ax = sns.countplot(data=**postcodedataset**, x = \"**var**\", color = \"**colourname**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What category are most of the postcodes in these urban areas located in?\n",
    "\n",
    "Is this expected?\n",
    "\n",
    "Are there any categories that contain no postcodes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average rate of broadband access by urban-rural classification\n",
    "\n",
    "Looking at the average rate of lack of access to broadband by classification using <code>DataFrame.groupby(_col\\_for\\_grouping_)[_col_].mean()</code>, save it to `ur_to_nbr` and plot it with `sns.barplot()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ur_to_nbr = **postcodedataset**.groupby(\"**groupvar**\")[\"**var**\"].mean()\n",
    "\n",
    "sns.barplot(x = ur_to_nbr.index, y = ur_to_nbr.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tidy up this graph so it is clearer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "ax = sns.barplot(x = ur_to_nbr.index, y = ur_to_nbr.values, color = \"**colourname**\")\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What is the difference between the average rate of no access to broadband for each of the values of urban-rural classification?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifing priority postcodes\n",
    "\n",
    "### Create a score\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "From the understanding of the variables carried out in the previous secction it can be seen that the postcodes most in need are those in the low SIMD bands, have schemes attached and are not in ur_class 1.\n",
    "\n",
    "To enable a prioritisation, it is possible to create a score for each individual postcode that takes into account these different factors.\n",
    "\n",
    "The different weightings applied to each variable can be changed dependent on identifying different priorities.\n",
    "\n",
    "An example score approach to creating a score is given below.\n",
    "\n",
    "For example, give:\n",
    "\n",
    "* SIMD score\n",
    "  + 10 points for the lowest SIMD decile upto 1 point for the highest decile.\n",
    "\n",
    "* ur_class score\n",
    "  + 10 points for ur_class = 6,\n",
    "  + 5 points for ur_class = 2 and 3\n",
    "  + 0 points for ur_class 1\n",
    "\n",
    "* scheme_references score\n",
    "  + 10 points for having a `scheme_references`\n",
    "  + 0 points for no scheme reference\n",
    "\n",
    "Adding up the points will give a total score for each postcode\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.assign()` create individual scores for each contributing factor of `simd10`, `ur_class` and `scheme_references` and a `total_score` that sums all three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_pc_score = cf_pc_analysis.assign(\n",
    "    simd_score=lambda df: 11 - df[\"simd10\"] ,\n",
    "    ur_score=lambda df: df[\"ur_class\"].transform(lambda c: 10 if (c == 6) else 5 if ((c == 2) or (c == 3)) else 0),\n",
    "    scheme_score=lambda df: df[\"scheme_references\"].transform(lambda sr: 10 if sr in [\"IVS\", \"MVS\"] else 0),\n",
    "    total_score=lambda df: df[\"simd_score\"] + df[\"ur_score\"] + df[\"scheme_score\"]\n",
    ")\n",
    "cf_pc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the score distribution\n",
    "\n",
    "<div class=\"alert alter-block alert-info\">\n",
    "\n",
    "The priority postcodes are now those with the highest scores.\n",
    "\n",
    "Th next step is to decide on a score cut-off for the prioritisation.\n",
    "\n",
    "To do this it is necessary to order the postcodes by descending score and visualise this distribution to identify which score to use as a cut-off.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the distribution of the `total_score` from highest to lowest\n",
    "\n",
    "Select the `postcode2` and `total_score` using <code>DataFrame[[_col1_, _col2_]]</code>.\n",
    "Using `DataFrame.sort_values()` sort the `total_score` in descending order from highest to lowest.\n",
    "\n",
    "It is now necessary to give this data frame a new index start counting from 0 to show how many postcodes are at each score for the distribution. Use `DataFrame.reset_index(drop=True)` to reset the index.\n",
    "\n",
    "Save this data frame to variable `score_dist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_dist = **scoredataset**[['**var1**', '**scorevar**']].sort_values(by='**scorevar**', ascending=False).reset_index(drop=True)\n",
    "score_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `sns.lineplot()` plot a line graph of `score_dist.index` vs `score_dist[\"total_score\"]` for the score distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x = score_dist.index, y = score_dist[\"total_score\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that only roughly 5000 postcodes have a score above 10.\n",
    "It is now useful to focus on the highest scoring 5000 postcodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify a score cut-off\n",
    "\n",
    "Using `head()`, extract the top 5000 postcodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_dist_top = score_dist.head(**number**)\n",
    "score_dist_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replot the distribution of the top scoring postcodes.\n",
    "Focus on making the graph clear and informative with titles and axis labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.lineplot(x = score_dist_top.index, y = score_dist_top[\"**scorevar**\"])\n",
    "ax.set_title(\"**cleartitle**\")\n",
    "ax.set_xlabel(\"**xaxislabel**\")\n",
    "ax.set_ylabel(\"**yaxislabel**\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "What score cut-off will identify roughly the top 1500 postcodes?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output the priority postcodes\n",
    "\n",
    "Create a new dataset with the postcodes with scores of 15 and over.\n",
    "Using boolean indexing to select all postcodes with a `total_score` of 15 and over.\n",
    "Keep `postcode2` and `total_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**prioritydataset** = score_dist_top[score_dist_top[\"**scorevar**\"] >= **cutoff**]\n",
    "**prioritydataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge in supporting information from the analysis dataset by performing left join using `pd.merge()`.\n",
    "\n",
    "This will be needed for the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "**outputdataset** = pd.merge(**prioritydataset**, **postcodedataset**, on=\"postcode2\", how=\"left\")\n",
    "**outputdataset** = **outputdataset**[[\"city\", \"postcode\", \"latitude\", \"longitude\",\n",
    "                           \"total_score\", \"scheme_references\", \"nobroadband_rate\",\n",
    "                           \"simd10\", \"pctype\", \"ur_class\", \"simd_rank\", \"postcode2\"]]\n",
    "**outputdataset**.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension - Prioritise scoring differently\n",
    "\n",
    "<div class=\"alert alter-block alert-warning\">\n",
    "\n",
    "Experiment with a different approach to creating a score. \n",
    "    \n",
    "Identify a different 1000 - 2000 postcodes to focus on.\n",
    "\n",
    "How much overlap is there between the first and second set?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension -  Further analysis ideas\n",
    "\n",
    "Use the postcode level population information from the 2011 census to maximise the population impacted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 8 - Plot prioritised postcodes on a Google map\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 8 - Google mapping of prioritised postcodes**\n",
    "\n",
    "This lesson covers the steps required to plot the location of the priority postcodes on a Google map. \n",
    "    \n",
    "The steps involved cover:\n",
    "\n",
    "* Export a file with the required pin information\n",
    "* Import this into google maps\n",
    "* Select the marker fields\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out a file with the required pin information for Google Maps to the output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gmaps = cf_pc_focus[[\"city\", \"postcode\", \"latitude\", \"longitude\", \"total_score\", \"scheme_references\", \"nobroadband_rate\", \"simd10\", \"pctype\", \"ur_class\"]]\n",
    "gmaps.to_csv(path.join(output_file_path, \"cf_prioritised_pc.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Go to [https://www.google.com/mymaps](https://www.google.com/mymaps)\n",
    "\n",
    "2. Select \"Create a new map\"\n",
    "\n",
    "3. Give the map a suitable title\n",
    "\n",
    "4. Import the exported gmaps `.csv` file\n",
    "\n",
    "5. Select latitude and longitude as the position fields\n",
    "\n",
    "6. Select Postcode as the marker title\n",
    "\n",
    "All other data will be imported and will be visible on the marker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Lesson 9 - interactive postcode plotting using leaflet\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "**Lesson 9 - interactive postcode mapping**\n",
    "\n",
    "This final lesson covers plotting the shape and information about the priority postcodes using a mapping solution.\n",
    "\n",
    "The purpose of this exercise is to be able to easily share the location and information about the priority postcodes and also to visually see where they are located.\n",
    "\n",
    "Are they all in one city?\n",
    "\n",
    "Are in the centres, or on the edges of the cities?\n",
    "\n",
    "The mapping activity will involve use of additional packages and an open source mapping tool called [Leaflet](https://leafletjs.com/) which is accessed through the use of the [Folium](https://python-visualization.github.io/folium/) package.\n",
    "\n",
    "The steps involved with mapping the postcodes onto a map include:\n",
    "\n",
    "* Read in the postcode boundaries\n",
    "* Convert coordinates to longitude and latitude\n",
    "* Merge in any additional data\n",
    "* Create pop-up labels for each postcode\n",
    "* Convert to a spatial file type\n",
    "* Import this information to the map\n",
    "* Save the map as an html file for sharing\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import postcode boundary files\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The geometry information for the postcodes is available in the postcode shape files.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `gpd.read_file()`, read in the postcode boundary shape file - \"PC_CUT_20_2.shp\" tidying up the column names on import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_geom_input = gpd.read_file(path.join(input_file_path, \"PC_CUT_20_2/PC_CUT_20_2.shp\")).clean_names(case_type='snake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `DataFrame.head()` have quick look at this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_geom_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that each row is a postcode and the boundaries are contained within the geometry information.\n",
    "The geometry information is a set of polygons defined by points.\n",
    "\n",
    "Subset the file to just retain only the postcode and geometry fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_boundaries = pc_geom_input[[\"postcode\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert coordinate system\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This is similar to a dataframe, but contains geometric information in a specific coordinate system.\n",
    "This file contains polygons defining the boundaries of each postcode.\n",
    "\n",
    "The coordinate system currently used is the OSGB 1936 /British National Grid, which maps the boundaries to Eastings and Northings.\n",
    "[Ordinance Survey National Grid](https://en.wikipedia.org/wiki/Ordnance_Survey_National_Grid)\n",
    "\n",
    "To use a mapping tool, these need to be converted to values of longitude and latitude.\n",
    "This can be done using a coordinate system transformation.\n",
    "The value required is 4326, which links to the [World Geodetic System](https://en.wikipedia.org/wiki/World_Geodetic_System)\n",
    "\n",
    "</div>\n",
    "\n",
    "Setting the coordinate reference system `crs` to 4326 using `GeoDataFrame.to_crs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_latlong = pc_boundaries.to_crs(epsg=4326)\n",
    "pc_latlong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in additional data\n",
    "\n",
    "Finally recreate the `postcode2` field to enable merging in of additional data items.\n",
    "Use `Series.str.upper()` for uppercase and `Series.str.replace(\" \", \"\")` to remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pc_latlong = pc_latlong.assign(postcode2 = pc_latlong[\"postcode\"].str.upper().str.replace(\" \", \"\"))\n",
    "pc_latlong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only the priority postcode geometry by performing left join using `pd.merge()` with the `cf_pc_focus` file\n",
    "\n",
    "Geometry is only available for \"small\" postcodes,\n",
    "so also filter out those that do not have any geometry using `DataFrame.dropna(subset=[\"geometry\"])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_focus_latlong = pd.merge(cf_pc_focus[[\"postcode2\", \"scheme_references\", \"ur_class\",\n",
    "                                         \"simd10\", \"simd_rank\", \"nobroadband_rate\"]],\n",
    "                            pc_latlong, on=\"postcode2\", how=\"left\")\n",
    "print(f\"{cf_focus_latlong.shape[0]} rows before dropping missing values\")\n",
    "cf_focus_latlong = cf_focus_latlong.dropna(subset=[\"geometry\"])\n",
    "print(f\"{cf_focus_latlong.shape[0]} rows after dropping missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tooltips\n",
    "\n",
    "Create a tooltip for each postcode on the map for both scoring approaches using `DataFrame.assign()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cf_focus_latlong = cf_focus_latlong.assign(tooltip = lambda df:\n",
    "                     \"<h5>\" + df[\"postcode\"] + '</h5>' + '<br />' +\n",
    "                     \"<b>Scheme: </b>\" + df[\"scheme_references\"] + \"<br />\" +\n",
    "                     \"<b>Urban Rural Classification: </b>\" + df[\"ur_class\"].apply(str) + \"<br />\" +\n",
    "                     \"<b>SIMD Decile: </b>\" + df[\"simd10\"].apply(str) + \"<br />\" +\n",
    "                     \"<b>SIMD Rank: </b>\" + df[\"simd_rank\"].apply(str) + \"<br />\" +\n",
    "                     \"<b>No broadband access rate: </b>\" + df[\"nobroadband_rate\"].apply(str) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create coordinates for the centre of the map to be around the centre of Scotland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "centre = [56.5, -4.2]  # [lat, long]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import into a leaflet map using Folium\n",
    "\n",
    "Draw the boundaries using the `folium` module.\n",
    "Full list of style arguments are available [here](https://leafletjs.com/examples/geojson/).\n",
    "\n",
    "<!-- Tutorial on using Folium with geopandas: https://geopandas.org/gallery/polygon_plotting_with_folium.html -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "map = folium.Map(location=centre, zoom_start=7)\n",
    "\n",
    "for _, r in cf_focus_latlong.iterrows():\n",
    "    sim_geo = gpd.GeoSeries(r['geometry']).simplify(tolerance=0.001)\n",
    "    geo_j = sim_geo.to_json()\n",
    "    geo_j = folium.GeoJson(data=geo_j,\n",
    "                           style_function=lambda x: {\n",
    "                               'stroke': True,\n",
    "                               'color': '#ff8c00',\n",
    "                               'fillColor': '#ff8c00',\n",
    "                               'opacity': 0.5,\n",
    "                               'fillOpacity': 0.5,\n",
    "                               'weight': 2\n",
    "                           },\n",
    "                           highlight_function=lambda x: {\n",
    "                               'stroke': True,\n",
    "                               'color': '#000000',\n",
    "                               'fillColor': '#ff8c00',\n",
    "                               'opacity': 0.8,\n",
    "                               'fillOpacity': 0.8,\n",
    "                               'bringToFront': True,\n",
    "                               'weight': 2\n",
    "                           })\n",
    "    folium.Tooltip(r['tooltip']).add_to(geo_j)\n",
    "    geo_j.add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to save the map for sharing as HTML, using `map.save()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.save(path.join(output_file_path, \"cityfibre.html\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension - Map experimentation\n",
    "\n",
    "Experiment with changing the information that is displayed in the tool tips.\n",
    "\n",
    "Reformat the tooltips.\n",
    "\n",
    "Change the colours that are used for background and highlight on the leaflet map.\n",
    "\n",
    "Change the dataset for the differently prioritised scoring dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
